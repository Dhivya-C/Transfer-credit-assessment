{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX8t6sCzigcO"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZu97Dn3ktRR"
      },
      "source": [
        "\"\"\"\n",
        "Author: Dhivya\n",
        "\n",
        "Description: Assessment of Transfer Credit using Verb Clustering\n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFO0ng-LycbQ"
      },
      "source": [
        "class Preprocessing(object):\n",
        "  def remove_empty(self, lo_list):\n",
        "    lo = [x for x in lo_list if x != 'nan']\n",
        "    return lo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXvauCde3oN4"
      },
      "source": [
        "class Taxonomic_Similarity(object):\n",
        "\n",
        "    \"\"\"[summary]\n",
        "\n",
        "    [description]\n",
        "    \"\"\"\n",
        "    def __init__(self,):\n",
        "      self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "      nltk.download('wordnet')\n",
        "      nltk.download('punkt')\n",
        "      self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def possible_verb(self, word): \n",
        "      s = set(s.pos() for s in wn.synsets(word))\n",
        "      if 'v' in s:\n",
        "        return True \n",
        "\n",
        "    def detect_verbs_spacy(self, text):\n",
        "      doc = self.nlp(text)\n",
        "      doc1 = word_tokenize(text)\n",
        "      verbs = []\n",
        "      for token in doc:\n",
        "          if token.pos_=='VERB':\n",
        "              if self.possible_verb(token.text):\n",
        "                verbs.append(token.text)                   \n",
        "      if not verbs:\n",
        "        verb2 = []\n",
        "        for token1 in doc1:\n",
        "          if self.possible_verb(token1):\n",
        "              verb2.append(token1)\n",
        "        return verb2\n",
        "      else:\n",
        "        return verbs\n",
        "      \n",
        "    def form_verb_clusters(self, verb_list):\n",
        "      knowledge_list = ['arrange','define','duplicate','label','list','match','memorize',\n",
        "                    'name','order','outline','recognize','relate','recall','repeat','reproduce','state']\n",
        "\n",
        "      comprehension_list = ['explain','paraphrase','classify','convert','defend','discuss','distinguish',\n",
        "                            'estimate','explain','express','extend','generalized','indicate','infer',\n",
        "                            'locate','predict','rewrite','review','translate']\n",
        "\n",
        "      application_list = ['use','compute','solve','demonstrate','discover','construct','compute','dramatize','apply',\n",
        "                          'change','employ','interpret','manipulate','modify','operate','practice','produce','schedule',\n",
        "                          'show','sketch','solve']\n",
        "\n",
        "      synthesis_list = ['analyze','create','design','hypothesize','invent','develop','arrange','assemble','collect','combine','comply',\n",
        "                  'devise','explain','formulate','generate','plan','rearrange','reconstruct','tell','synthesize','revise','reorganize']\n",
        "\n",
        "      evaluation_list = ['judge','recommend','critique','justify','appraise','argue','assess','attach','conclude','defend','discriminate',\n",
        "                        'estimate','evaluate','explain','interpret','select','support','predict','relate','rate','value']\n",
        "\n",
        "      cluster_list = [knowledge_list,comprehension_list,application_list,synthesis_list,evaluation_list]\n",
        "      for i in range(len(verb_list)):\n",
        "          verb_list[i]= self.lemmatizer.lemmatize(verb_list[i], wn.VERB)\n",
        "      class_list = []\n",
        "      for i in range(len(verb_list)):\n",
        "          if verb_list[i] in knowledge_list:\n",
        "                  class_list.append(1)\n",
        "          elif  verb_list[i] in comprehension_list:\n",
        "                  class_list.append(2)\n",
        "          elif  verb_list[i] in application_list:\n",
        "                  class_list.append(3)\n",
        "          elif  verb_list[i] in synthesis_list:\n",
        "                  class_list.append(4)\n",
        "          elif  verb_list[i] in evaluation_list:\n",
        "                  class_list.append(5)\n",
        "          else:\n",
        "                  cluster_id = self.find_verb_cluster(verb_list[i],cluster_list)\n",
        "                  class_list.append(cluster_id)\n",
        "      return class_list\n",
        "\n",
        "    def verb_similarity(self, v1, v2):\n",
        "      sim1 = []\n",
        "      synset1 = wn.synsets(v1,pos='v')\n",
        "      synset2 = wn.synsets(v2,pos='v')\n",
        "      w1 = synset1[0]\n",
        "      w2 = synset2[0]\n",
        "      for syn1 in synset1:\n",
        "            for syn2 in synset2:\n",
        "                sim1.append(syn1.wup_similarity(syn2))              \n",
        "      wup_max_sim = max(sim1)\n",
        "      return wup_max_sim     \n",
        "    \n",
        "    def find_verb_cluster(self, new_verb,cluster_list):\n",
        "      avg_val_list = []\n",
        "      sil_width_list = []\n",
        "      for cluster in cluster_list:\n",
        "            avg_val = 0\n",
        "            sim_list = []\n",
        "            for verb in cluster:\n",
        "                sim = self.verb_similarity(new_verb,verb)\n",
        "                if sim==None:\n",
        "                  sim = 0\n",
        "                sim_list.append(sim)\n",
        "            avg_val = np.sum(sim_list)/len(sim_list)\n",
        "            avg_val_list.append(avg_val)\n",
        "      for value in avg_val_list:\n",
        "            rem_clusters = avg_val_list.copy()\n",
        "            rem_clusters.remove(value)\n",
        "            neig_cluster = min(rem_clusters)\n",
        "            sil_width = (neig_cluster - value) / max(neig_cluster,value)\n",
        "            sil_width_list.append(sil_width)\n",
        "      cluster_ind = sil_width_list.index(max(sil_width_list))\n",
        "      return cluster_ind+1\n",
        "\n",
        "    def assign_cluster(self, verb_list):\n",
        "        cluster_list = []\n",
        "        print(len(verb_list))\n",
        "        for i in range(len(verb_list)): \n",
        "            clu = []\n",
        "            if (verb_list[i]==[]):\n",
        "                clu = [0]\n",
        "            else:\n",
        "                #print(verb_list[i])\n",
        "                clu = self.form_verb_clusters(verb_list[i])\n",
        "            cluster_list.append(clu)\n",
        "        cluster = [max(x) for x in cluster_list]\n",
        "        return cluster\n",
        "\n",
        "    def shift_grid(self, rlo_cluster,slo_cluster,rl,sl):\n",
        "          print(rlo_cluster,slo_cluster)\n",
        "          shift_list =[]\n",
        "          counter = 0\n",
        "          for i in rlo_cluster:\n",
        "              tmp_val = []\n",
        "              for j in slo_cluster:\n",
        "                  counter = counter +1\n",
        "                  # print(counter)\n",
        "                  shift_value = abs(i-j)\n",
        "                  tmp_val.append(shift_value)\n",
        "              shift_list.append(tmp_val)\n",
        "          df1 = pd.DataFrame(shift_list)\n",
        "          df1['index'] = rl\n",
        "          df1.set_index('index',inplace=True)\n",
        "          df1.columns = sl\n",
        "          return df1, shift_list\n",
        "      \n",
        "    def shift_grid(self, rlo_cluster,slo_cluster,rl,sl):\n",
        "        print(rlo_cluster,slo_cluster)\n",
        "        shift_list =[]\n",
        "        counter = 0\n",
        "        for i in rlo_cluster:\n",
        "            tmp_val = []\n",
        "            for j in slo_cluster:\n",
        "                counter = counter +1\n",
        "                shift_value = abs(i-j)\n",
        "                tmp_val.append(shift_value)\n",
        "            shift_list.append(tmp_val)\n",
        "        df1 = pd.DataFrame(shift_list)\n",
        "        df1['index'] = rl\n",
        "        df1.set_index('index',inplace=True)\n",
        "        df1.columns = sl\n",
        "        return df1, shift_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2hQ7sAY2nUt"
      },
      "source": [
        "class Semantic_Similarity(object):\n",
        "\n",
        "  def __init__(self,):\n",
        "    self.model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
        "  \n",
        "  def calculate_similarity(self, s1, s2):\n",
        "    sentence_vec1 = self.model.encode(s1, convert_to_tensor=True)\n",
        "    sentence_vec2 = self.model.encode(s2, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(sentence_vec1, sentence_vec2)\n",
        "    cosine_scores = cosine_scores.cpu().numpy()\n",
        "    return cosine_scores[0][0]\n",
        "  \n",
        "  def similarity_grid(self, sent_1,sent_2):\n",
        "    sim_list = []\n",
        "    df = pd.DataFrame()\n",
        "    df['index'] = sent_1\n",
        "    df.set_index('index',inplace=True)\n",
        "    for w1 in sent_2:\n",
        "        temp_val = []\n",
        "        for w2 in sent_1:\n",
        "            sim_value = self.calculate_similarity(w1,w2)\n",
        "            temp_val.append(sim_value)\n",
        "        sim_list.append(temp_val)\n",
        "        df[w1] = temp_val\n",
        "    return df, sim_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS1nAwkH66FH"
      },
      "source": [
        "class Aggregation(object):\n",
        "  def final_similarity(self,similarity_list,st_list,impact):\n",
        "    final_sim_list = []\n",
        "    for m in range(len(similarity_list)):\n",
        "      temp_sim = []\n",
        "      for n in range(len(similarity_list[m])):\n",
        "          lo_sim = similarity_list[m][n]*(100-impact)\n",
        "          cluster_sim = impact-((impact/6)*st_list[m][n])\n",
        "          final_sim = lo_sim + cluster_sim\n",
        "          temp_sim.append(final_sim)\n",
        "      final_sim_list.append(temp_sim)\n",
        "    final_sim_list = [list(x) for x in zip(*final_sim_list)]\n",
        "    return final_sim_list\n",
        "\n",
        "  def course_similarity(self, final_sim_list,sim_threshold):\n",
        "    lo_sim_category = [False]*len(final_sim_list)\n",
        "    for i in range(len(final_sim_list)):\n",
        "        if max(final_sim_list[i])>sim_threshold : lo_sim_category[i]=True\n",
        "    true_count = lo_sim_category.count(True)\n",
        "    false_count = lo_sim_category.count(False)\n",
        "    print(lo_sim_category)\n",
        "    if true_count >= len(lo_sim_category)/2:\n",
        "        course_sim = 'Similar'\n",
        "    else: \n",
        "        course_sim = 'Not Similar'\n",
        "    return course_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKovNQN5x_m3"
      },
      "source": [
        "def test_function():\n",
        "  los = pd.read_csv('/content/drive/MyDrive/course_comparisons/course2.csv')\n",
        "  impact = 30\n",
        "  sim_threshold=60\n",
        "  rlo = list(map(str,los['RLO'].tolist()))\n",
        "  slo = list(map(str,los['SLO'].tolist()))\n",
        "  preprocessing_object = Preprocessing()\n",
        "  tax_object = Taxonomic_Similarity()\n",
        "  sem_object = Semantic_Similarity()\n",
        "  agg_object = Aggregation()\n",
        "\n",
        "  rlo = preprocessing_object.remove_empty(rlo)\n",
        "  slo = preprocessing_object.remove_empty(slo)\n",
        "\n",
        "  rlo_verb_list = [tax_object.detect_verbs_spacy(x) for x in rlo]\n",
        "  slo_verb_list = [tax_object.detect_verbs_spacy(x) for x in slo] \n",
        "\n",
        "  rlo_cluster = tax_object.assign_cluster(rlo_verb_list)\n",
        "  slo_cluster = tax_object.assign_cluster(slo_verb_list)\n",
        "\n",
        "  st_grid, st_list = tax_object.shift_grid(rlo_cluster,slo_cluster,rlo,slo)\n",
        "  st_list = [list(x) for x in zip(*st_list)]\n",
        "  \n",
        "  sim_grid, similarity_list = sem_object.similarity_grid(rlo,slo)\n",
        "  final_sim_list = agg_object.final_similarity(similarity_list,st_list,impact)\n",
        "  result = agg_object.course_similarity(final_sim_list,sim_threshold)\n",
        "  print(\"The two courses taken into consideration are \" + result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hngmwRrJ3vRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e792bbb-997b-4896-d8ff-70e26aed2b8c"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    test_function()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "7\n",
            "9\n",
            "[2, 4, 5, 4, 4, 5, 4] [2, 4, 4, 4, 5, 4, 5, 4, 5]\n",
            "[True, False, True, True, False, True, False]\n",
            "The two courses taken into consideration are Similar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JhiDSwbXIvD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}