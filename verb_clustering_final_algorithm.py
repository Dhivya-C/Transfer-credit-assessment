# -*- coding: utf-8 -*-
"""Verb_clustering_final_algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQHwCix-waOQDkNBqOG_3Ik_xhCqP8zm
"""

!pip install -U sentence-transformers

import pandas as pd
import spacy
import nltk
nlp = spacy.load("en_core_web_sm")
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
from nltk.corpus import wordnet as wn
from nltk.tokenize import word_tokenize
nltk.download('wordnet')
nltk.download('punkt')
import numpy as np
from sentence_transformers import SentenceTransformer
from sentence_transformers import util

los = pd.read_csv('/content/drive/MyDrive/course_comparisons/course2.csv')

los

def possible_verb(surface): 
    s = set(s.pos() for s in wn.synsets(surface))
    if 'v' in s:
      return True

def detect_verbs_spacy(text):
    doc = nlp(text)
    doc1 = word_tokenize(text)
    verbs = []
    for token in doc:
        if token.pos_=='VERB':
            if possible_verb(token.text):
              verbs.append(token.text)                   
    if not verbs:
      #  print('list is empty')
       verb2 = []
       for token1 in doc1:
         if possible_verb(token1):
            # print(token1)
            verb2.append(token1)
       return verb2
    else:
       return verbs

rlo = list(map(str,los['RLO'].tolist()))
slo = list(map(str,los['SLO'].tolist()))

rlo

slo

def remove_empty(lo_list):
    lo = [x for x in lo_list if x != 'nan']
    return lo

rlo = remove_empty(rlo)
slo = remove_empty(slo)

rlo_verb_list = [detect_verbs_spacy(x) for x in rlo]
slo_verb_list = [detect_verbs_spacy(x) for x in slo]

def form_verb_clusters(verb_list):

    knowledge_list = ['arrange','define','duplicate','label','list','match','memorize',
                  'name','order','outline','recognize','relate','recall','repeat','reproduce','state']

    comprehension_list = ['explain','paraphrase','classify','convert','defend','discuss','distinguish',
                          'estimate','explain','express','extend','generalized','indicate','infer',
                          'locate','predict','rewrite','review','translate']

    application_list = ['use','compute','solve','demonstrate','discover','construct','compute','dramatize','apply',
                        'change','employ','interpret','manipulate','modify','operate','practice','produce','schedule',
                        'show','sketch','solve']

    synthesis_list = ['analyze','create','design','hypothesize','invent','develop','arrange','assemble','collect','combine','comply',
                'devise','explain','formulate','generate','plan','rearrange','reconstruct','tell','synthesize','revise','reorganize']

    evaluation_list = ['judge','recommend','critique','justify','appraise','argue','assess','attach','conclude','defend','discriminate',
                      'estimate','evaluate','explain','interpret','select','support','predict','relate','rate','value']

    cluster_list = [knowledge_list,comprehension_list,application_list,synthesis_list,evaluation_list]
    for i in range(len(verb_list)):
        verb_list[i]= lemmatizer.lemmatize(verb_list[i], wn.VERB)
    class_list = []
    for i in range(len(verb_list)):
        #print(verb_list[i])
        if verb_list[i] in knowledge_list:
                class_list.append(1)
        elif  verb_list[i] in comprehension_list:
                class_list.append(2)
        elif  verb_list[i] in application_list:
                class_list.append(3)
        elif  verb_list[i] in synthesis_list:
                class_list.append(4)
        elif  verb_list[i] in evaluation_list:
                class_list.append(5)
        else:
                cluster_id = find_verb_cluster(verb_list[i],cluster_list)
                class_list.append(cluster_id)
    return class_list

def verb_similarity(v1,v2):
    sim1 = []
    synset1 = wn.synsets(v1,pos='v')
    synset2 = wn.synsets(v2,pos='v')
    w1 = synset1[0]
    w2 = synset2[0]
    for syn1 in synset1:
          for syn2 in synset2:
              sim1.append(syn1.wup_similarity(syn2))              
    wup_max_sim = max(sim1)
    return wup_max_sim

def find_verb_cluster(new_verb,cluster_list):
    avg_val_list = []
    sil_width_list = []
    for cluster in cluster_list:
          avg_val = 0
          sim_list = []
          for verb in cluster:
              sim = verb_similarity(new_verb,verb)
              if sim==None:
                sim = 0
              #print(sim)
              sim_list.append(sim)
              #print(sim_list)
          avg_val = np.sum(sim_list)/len(sim_list)
          #print("The average value is ", avg_val)
          avg_val_list.append(avg_val)
    for value in avg_val_list:
          rem_clusters = avg_val_list.copy()
          #print(rem_clusters)
          rem_clusters.remove(value)
          #print(rem_clusters)
          neig_cluster = min(rem_clusters)
          sil_width = (neig_cluster - value) / max(neig_cluster,value)
          sil_width_list.append(sil_width)
    #print(sil_width_list)
    cluster_ind = sil_width_list.index(max(sil_width_list))
    return cluster_ind+1

def assign_cluster(verb_list):
    cluster_list = []
    print(len(verb_list))
    for i in range(len(verb_list)): 
        clu = []
        if (verb_list[i]==[]):
            clu = [0]
        else:
            #print(verb_list[i])
            clu = form_verb_clusters(verb_list[i])
        cluster_list.append(clu)
    cluster = [max(x) for x in cluster_list]
    return cluster

rlo_cluster = assign_cluster(rlo_verb_list)
slo_cluster = assign_cluster(slo_verb_list)

slo_tuple = list(zip(slo,slo_cluster))

slo_tuple

rlo_tuple = list(zip(rlo,rlo_cluster))

rlo_tuple

slo_tuple

def similarity_grid(sent_1,sent_2):
    sim_list = []
    df = pd.DataFrame()
    df['index'] = sent_1
    df.set_index('index',inplace=True)
    for w1 in sent_2:
        temp_val = []
        for w2 in sent_1:
            sim_value = calculate_similarity(w1,w2)
            temp_val.append(sim_value)
        sim_list.append(temp_val)
        df[w1] = temp_val
    return df, sim_list

def calculate_similarity(s1, s2):
  sentence_vec1 = model.encode(s1, convert_to_tensor=True)
  sentence_vec2 = model.encode(s2, convert_to_tensor=True)
  cosine_scores = util.pytorch_cos_sim(sentence_vec1, sentence_vec2)
  cosine_scores = cosine_scores.cpu().numpy()
  return cosine_scores[0][0]

def shift_grid(rlo_cluster,slo_cluster,rl,sl):
    print(rlo_cluster,slo_cluster)
    shift_list =[]
    counter = 0
    for i in rlo_cluster:
        tmp_val = []
        for j in slo_cluster:
            counter = counter +1
            # print(counter)
            shift_value = abs(i-j)
            tmp_val.append(shift_value)
        shift_list.append(tmp_val)
    df1 = pd.DataFrame(shift_list)
    df1['index'] = rl
    df1.set_index('index',inplace=True)
    df1.columns = sl
    return df1, shift_list

model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')

sim_grid, similarity_list = similarity_grid(rlo,slo)

st_grid, st_list = shift_grid(rlo_cluster,slo_cluster,rlo,slo)

st_list = [list(x) for x in zip(*st_list)]

sim_grid

st_grid

x = 30
final_sim_list = []
for m in range(len(similarity_list)):
    temp_sim = []
    for n in range(len(similarity_list[m])):
        lo_sim = similarity_list[m][n]*(100-x)
        cluster_sim = x-((x/6)*st_list[m][n])
        final_sim = lo_sim + cluster_sim
        temp_sim.append(final_sim)
    final_sim_list.append(temp_sim)

final_sim_list = [list(x) for x in zip(*final_sim_list)]

df_final = pd.DataFrame(final_sim_list)
df_final['index'] = rlo
df_final.set_index('index',inplace=True)
df_final.columns = slo

df_final

def aggregation(sim_list_list):
    lo_sim_category = [False]*len(sim_list_list)
    for i in range(len(final_sim_list)):
        # print(final_sim_list[i])
        if max(final_sim_list[i])>60 : lo_sim_category[i]=True
    true_count = lo_sim_category.count(True)
    false_count = lo_sim_category.count(False)
    print(lo_sim_category)
    if true_count >= len(lo_sim_category)/2:
        course_sim = 'Similar'
    else: 
        course_sim = 'Not Similar'
    return course_sim

result = aggregation(final_sim_list)

print("The two courses taken into consideration are " + result)

